---
title: "forceR"
output: rmarkdown::html_vignette
# output: pdf_document
vignette: >
  %\VignetteIndexEntry{forceR}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup,  include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

\begin{center}
Peter T. Rühr

Bonn, `r format(Sys.time(), '%B %Y')` 
\end{center}

## Introduction
The package `forceR` has originally been written for insect bite force data preparation and analysis, but it can be used for any kind of time series measurements. Functions include 

* loading, plotting, and cropping of data
* correction of charge amplifier drifts
* correction of baseline drifts
* reduction of sampling frequency
* automatic extraction of single peaks
* rescaling (normalization) of curves
* reduction of curves to 100 time steps each
* finding of best polynomial fits to describe all curves
<!-- * preparation and export of curves for subsequent group-wise analyses of group-specific mean peak curves via 1-dimensional statistical parametric mapping in `Python`. -->

This vignette guides you through all functions of the package. A limited data set of 8 bite force measurement files is used to get the ideas across while allowing for fast calculations.

At the end of this vignette you will find a self-sufficient workflow example that runs all `forceR` commands after file loading and drift corrections. Instead of loading files, bite series are simulated and then analsed.

Please cite the following publicaiton when using the `forceR` package:

Rühr, PT & Blanke, A (**in rev.**): `forceX` and `forceR`: a mobile setup and R package to measure and analyze a wide range of animal closing forces. *Methods in Ecology and Evolution* **XXX**: pp.XX-XX. doi: XXX

## Installation
# Official release
You can install the official releas of `forceR` from [CRAN](xxx):
```{r warning=FALSE, message=FALSE}
# install.packages('forceR') not yet
```

# Development version
You can install the development version of `forceR` from [GitHub](https://github.com/Peter-T-Ruehr/forceR):
```{r warning=FALSE, message=FALSE, eval=F}
require(devtools)
devtools::install_github("https://github.com/Peter-T-Ruehr/forceR")
```

## Load necessary packages, including `forceR`
```{r warning=FALSE, message=FALSE}
library(magrittr)
library(dplyr)
library(stringr)
library(purrr)
library(ggplot2)
library(readr)

library(forceR)
```

### Download raw measurement files
From hereon, everything will happen within a folder stored within the variable `data.folder`. Please download and unzip the content of the [example_data.zip file](https://github.com/Peter-T-Ruehr/forceR-data/blob/main/example_data.zip) onto your hard drive and store the folder location in the variable `data.folder`. This folder contains every file that is needed and has been produced during the writing of this vignette, so you can always restore the files from `GitGub` in case you have accidentally overwritten anything you wanted to keep.

First, we set the folder containing the example data as the `data.folder` (make sure this fits your local path):

```{r eval=FALSE, warning=FALSE, message=FALSE}
data.folder <- "./example_data"
```

```{r eval=TRUE, warning=FALSE, message=FALSE, include=F}
data.folder <- "C:/Users/pruehr.EVOLUTION/Documents/forceR_bkp_2022-03-01/vignettes/example_data"
```

In case you want to use your own measurements, *and if they were recorded by the LJStream software (LabJack Corporation, Lakewood, Colorado, US; tested for v1.19) as suggested in the forceX instructions*, you can use the convert_measurement() function of `forceR` to convert your files and save them in the `data.folder`.

## Plot Raw Measurement
The measurement file should have the time steps (preferably in m.secs, since all code of this package expects this) in the first column, and the measurements in the second column. Column names do not matter, and everything but the first two columns will be ignored.

```{r eval=TRUE, warning=FALSE, message=FALSE, fig.width = 6, fig.height=4}
file <- file.path(data.folder, "0982.csv")
plot_measurement(file,
                 columns = c(1:2))
```

## Crop Raw Measurement
```{r eval=FALSE, warning=FALSE, message=FALSE}
crop_measurement(file)
```
Select two points at the beginning and end of the part of the measurement we want to keep. If more than two points are selected, only the last two points will be considered. This allows for corrections of erroneous clicks. The position of the points on the y-axis is irrelevant. Make sure that the start and end points are placed on the graph between peaks, i.e. on the baseline. The y-values of the graph at these positions will be used as reference points in the following `amp_drift_corr()` funciton.

![](C:/Users/pruehr.EVOLUTION/Documents/forceR_bkp_2022-03-01/vignettes/figures/cropped.png){width=6in}

The resulting file will be saved in the subfolder `./cropped` with the suffix `"_cropped"`. We can plot it again:

```{r eval=TRUE, warning=FALSE, message=FALSE, fig.width = 6, fig.height=4}
cropped.folder <- file.path(data.folder, "cropped")

file <- file.path(cropped.folder, "0982_cropped.csv")
plot_measurement(file)
```

## Amplifier Drift Correction
To remove the systemic, asymptotical drift characteristic for charge amplifiers with resistor–capacitor (RC) circuits, we can use the `amp_drift_corr()` function. This function requires a few parameters. `?amp_drift_corr` brings up the following explanation:

| parameter | function |
| -- | ----------- |
| folder | Path to folder containing measurements |
| tau | Numeric time constant of charge amplifier in the same time unit as the measurement data. Default: `9400` |
| print.to.screen | A logical value indicating if results should be plotted in the current R plot. device. Slows down process. Default: `FALSE`. |
| print.to.pdf | A logical value indicating  if results should be saved as PDFs. Does not slow down the process as much as printing to the R plot device and is considered necessary to quality check the results. Default: `TRUE`. |
| res.reduction | A numeric value to reduce the number of time steps by during plotting. Speeds up the plotting process and reduces PDF size. Has no effect on the results, only on the plots. Default: `10`. |
| start.file.number | A numeric value indicating at which file the loop should start. Helpful in case the loop stopped during a previous run. Default: `1`. |

Basically, we only have to decide how we want to plot the data, define the correct folder with our measurement data and provide the time constant (tau, $\tau$) of our charge amplifier. This function, in contrast to the previous functions, takes a folder as input because when the same charge amplifier was used for all measurements, the same settings can be used for all measurement files.

```{r eval=FALSE, warning=FALSE, message=FALSE}
amp_drift_corr(folder = cropped.folder)
```

This creates some new folders:

* `"./ampdriftcorr/"` to store the corrected CSV-files.
* `"./ampdriftcorr/logs/"` to store log files containing info on the `tau` used and the script version.
* `"./ampdriftcorr/pdfs/"` to store the PDFs of the time series before and after the correction (if `print.to.pdf == TRUE`).

The resulting PDF figure of one corrected measurement will look like this:

![](C:/Users/pruehr.EVOLUTION/Documents/forceR_bkp_2022-03-01/vignettes/figures/ampdriftcorr_0982.png){width=6in}

In gray, we see the raw mearuements, and in green the corrected data. Sometimes, there is an additional linear drift in the data, as indicated by the bright green graph that follows the upward linear slope of the orange line in the next figure. This occurs in case the first value of the measurement is not exactly `0` and is also corrected by the function. The final result is plotted as the dark green graph and saved in a new CSV file in the `"./ampdriftcorr/"` folder.

![](C:/Users/pruehr.EVOLUTION/Documents/forceR_bkp_2022-03-01/vignettes/figures/ampdriftcorr_0980.png){width=6in}


## Automatic or Manual Baseline Correction of Time Series
If the baseline (zero-line) of a measurement is unstable (e.g. due to temperature fluctuations, wind, ...), it needs to be continually adjusted throughout the measurement. Ideally, this is done automatically 
by the function `baseline.corr()`. This automatic adjustment of the baseline invokes a sliding window approach, during which the 'minimum' within each window is stored. A 'minimum' is defined by the by the parameter `quantile.size`. If `quantile.size` is set to `0.05`, the value below which 5% of the measurement data within the current window lies, is treated as the current window's 'minimum'. Not taking the actual minimum within each window prevents the treatment of short undershoots (artefacts created during the measurement) as minima. In a second iteration, another sliding window calculates the average of the 'minima' within each window. The resulting 'minimal' values are subtracted from the original time series.

This approach works well for time series with relatively short peaks. For longer peaks, the `window.size.mins` should be increased. However, the greater `window.size.mins` gets, the smaller becomes the baseline-correcting effect of the function.

For the file `"./example_data/cropped/ampdriftcorr/1068_cropped_ampdriftcorr.csv"`, we choose a `window.size.mins` of 2000:

```{r eval=FALSE, warning=FALSE, message=FALSE, fig.width = 6, fig.height=4}
ampdriftcorr.folder <- file.path(data.folder, "cropped/ampdriftcorr")

file = file.path(ampdriftcorr.folder, "1068_cropped_ampdriftcorr.csv")

plot_measurement(file)

baseline_corr(file, 
              corr.type = "auto",  
              print.to.screen = TRUE, 
              print.to.pdf = TRUE, 
              window.size.mins = 2000,
              window.size.means = NULL,
              quantile.size = 0.05,
              y.scale = 0.5,
              res.reduction = 10,
              Hz = 100)
```
```
[1] "1068_cropped_ampdriftcorr"
[1] "You selected automatic drift correction:"
[1] "sliding minima window size: 2000"
[1] "sliding means of minima window size: 2000"
[1] "calculating with 100 Hz."
[1] "Finding 5th percentile sliding minima..."
100%...[1] "Finding sliding means..."
```

Again, new folders, PDFs and log files are created. The resulting plot shows the original data in gray, the 'minima' of the sliding windows in blue, the sliding mean of the 'minima' in bright green and the corrected time series in dark green:

![](C:/Users/pruehr.EVOLUTION/Documents/forceR_bkp_2022-03-01/vignettes/figures/baselinecorr_auto.png){width=6in}

If the automatic approach does not yield acceptable results, an interactive manual approach to correct the baseline can be performed instead. We do the manual correction for the file `"./example_data/cropped/ampdriftcorr/1174_cropped_ampdriftcorr.csv"`, which contains measurements with long, plateau-like peaks where the automatic baseline correction would fail. 

```{r eval=FALSE, warning=FALSE, message=FALSE, fig.width = 6, fig.height=4}
file = file.path(ampdriftcorr.folder, "1174_cropped_ampdriftcorr.csv")

plot_measurement(file)

baseline_corr(file, 
              corr.type = "manual",  
              print.to.screen = TRUE, 
              print.to.pdf = TRUE)
```
```
[1] "1174_cropped_ampdriftcorr"
[1] "You selected manual drift correction."
[1] "Please select baseline points and click \"Finish\"."
[1] "31 line points taken."
[1] "Done!"
```

![](C:/Users/pruehr.EVOLUTION/Documents/forceR_bkp_2022-03-01/vignettes/figures/baselinecorr_man.png){width=6in}

The green line represents a spline along the 31 manually defined points and is subtracted from the original data (gray). The corrected data is plotted in dark green again.

## File Sorting
After all original files have been processed, we need to separate the resulting files that we want to keep from files that have been created along the way. The function `sort_files()` looks through a vector of folders (`data.folders`) to identify files with the same measurement name (i.e., the same character string in the file names before the first underscore (`_`) and keeps the version of the file that is found in the folder that is most closely to the end of the list of `data.folders`. Thus, it is important that the folders are parsed in the correct order. In our example, this order would look like this:

1. `"./example_data/"`
2. `"./example_data/cropped/"`
3. `"./example_data/cropped/ampdriftcorr/"`
4. `"./example_data/cropped/ampdriftcorr/baselinecorr"`

If a file is present in `"./example_data/cropped/ampdriftcorr/baselinecorr"`, this will be copied (`move = FALSE`) or moved (`move = TRUE`) to a folder specified in `path.corrected`, e.g., `"./example_data/corrected/"`. Versions of the same measurement in the other folders will be ingorded. This is iteratively repeated for the rest of the folders in reverse order of the `data.folders` vector.

```{r eval=TRUE, warning=FALSE, message=FALSE}
data.folders <- c(data.folder,
                  file.path(data.folder, "/cropped"),
                  file.path(data.folder, "/cropped/ampdriftcorr"),
                  file.path(data.folder, "/cropped/ampdriftcorr/baselinecorr"))
results.folder <- file.path(data.folder, "/corrected/")
```
```{r eval=FALSE, warning=FALSE, message=FALSE}
sort_files(data.folders = data.folders,
           results.folder = results.folder,
           move = F)
```
```
[1] "copying 1068_cropped_ampdriftcorr_baselinecorr.csv..."
[1] "copying 1174_cropped_ampdriftcorr_baselinecorr.csv..."
[1] "copying 0980_cropped_ampdriftcorr.csv..."
[1] "copying 0982_cropped_ampdriftcorr.csv..."
```

## File Loading
Now we will load the first file in the `results` folder using `load_single()`. The function removes all columns except the one defined in `columns = c(1:2)` and renames them to `t` and `y`. Then, it adds a `filename` column based on the file name of the data.

```{r eval=TRUE, warning=FALSE, message=FALSE}
file.list <- list.files(results.folder, pattern = "csv", full.names = TRUE)
df.1 <- load_single(file = file.list[1],
                    columns = c(1:2))
df.1
unique(df.1$filename)
```

Alternatively, We can rapidly load all files in the `results` folder by using `load_mult()`:
```{r eval=TRUE, warning=FALSE, message=FALSE}
df.all <- load_mult(folder = results.folder,
                    columns = c(1:2))
df.all
unique(df.all$filename)
```

The new tibble contains the measurement of all files in the selected folder.

In case you do not have your own data ready yet, you can create you own bite series with the `sumulate_bites()` function. Refer to its help file for details (`?sumulate_bites()`).

## Sampling Frequency Reduction
To reduce the sampling frequency of all measurements that are above the desired frequency of e.g. `200 Hz`, we can use the function `reduce.frq()`:

```{r eval=TRUE, warning=FALSE, message=FALSE}
df.all.200 <- reduce_frq(df = df.all,
                         Hz = 200,
                         measurement.col = "filename")
df.all.200
```

If `measurement.col` is not defined, the whole input data frames will be treated as if it was just one single time series. This is okay for data frames like `df.1`, but for data frames with multiple time series (like `df.all`), we need to define a grouping column, in our case the column `filename`.

## Convert Measurement to Force
To convert a voltage measurement to a force measurement in Newton, we need an amplification value and, depending on the measurement setup, the lever ratio of the lever forwarding the force from the force plate to the sensor. For this, we need a `classifier` in which these data are stored:


```{r eval=TRUE, warning=FALSE, message=FALSE}
# create classifier with `amp` and `lever.ratio` columns:
classifier <- tibble(measurement = c("0979", "0980", "0981", "0982", "1041", "1068", "1174", "1440"),
                     specimen = c("a","a","b","c","d","e","f","g"),
                     species = c("A","A","A","A","B","B","C","C"),
                     amp = c(2, 2, 2, 2, 2, 2, 0.5, 0.5),
                     lever.ratio = c(0.525, 0.525, 0.525, 0.525, 0.525, 0.525, 0.525, 0.525)) %>% 
  # sort columns
  select(species, specimen, measurement, amp, lever.ratio)
classifier

```

To link info from the classifier and the `df.all.200` tibble, we add the column `measurement` to the latter:

```{r eval=TRUE, warning=FALSE, message=FALSE}
# reduce filename to measurement number (number before first underscore):
df.all.200 <- df.all.200 %>% 
  mutate(measurement = gsub("(^[^_])*_.*", "\\1", filename))
head(df.all.200)
```

Now we can convert the y-values to force data and add the `specimen` and `species` info from the `classifier`:

```{r eval=TRUE, warning=FALSE, message=FALSE}
df.all.200.tax <- y_to_force(df = df.all.200, 
                             classifier = classifier, 
                             measurement.col = "measurement")
head(df.all.200.tax)
```

## Mininum, Maximum and Standard Deviation of Force Data per Measurement and Species
### Initial reduction to one row per specimen
Finally, we can start with the actual analysis of the data. First, we want to calculate the mininum, maximum and standard deviation of force for each measurement and, in case there were several measurements per specimen, also for each specimen. 

From the previous steps, df.all.200.tax already contains the columns `measurement`, `specimen` and `species`. Now, we can use the function `summarize_measurements()` to create a summary tibble by adding the column names for which we want a summary in `var1` and `var2`. `var1` must name the column that stores the unique measurement IDs, e.g. measurement number, to calculate minimal and maximal force values per measurement. As `var2` we will use 'specimen' to calculate the summary data per specimen from the min. and max. values of the measurements of that specimen. Thus, the resulting tibble will contain summaries of all measurements that were performed with the same specimen.

```{r eval=TRUE, warning=FALSE, message=FALSE}
var1 = "measurement"
var2 = "specimen"
df.summary.specimen <- summarize_measurements(df.all.200.tax, 
                                              var1, 
                                              var2)
df.summary.specimen

# boxplot of maximum force in specimens
ggplot(data = df.summary.specimen, mapping = aes(x=specimen,y=max.F.measurement)) +
  geom_jitter(aes(color='blue'),alpha=0.7) +
  geom_boxplot(fill="bisque",color="black",alpha=0.3) +
  # scale_y_log10() +
  labs(y="max(F)/specimen") +
  guides(color=FALSE) +
  theme_minimal()

```

### Further summary calculations for species-wise info
To get a summary on species-wise info, we have to calculate with the summarized specimen info. We are not using the `summarize_measurements()` functions because this would ignore the fact the some measurements of one species may come from the same specimen, but we only want to consider one maximum force value per specimen and not one per measurement.

```{r eval=TRUE, warning=FALSE, message=FALSE}
df.summary.species <- df.summary.specimen %>%
  # find max Fs of species
  group_by(species) %>%
  # calculate force values for each species
  mutate(max.F.species = max(max.F.specimen),
         mean.F.species = round(mean(max.F.specimen),6),
         sdv.max.F.species = sd(max.F.specimen)) %>% 
  ungroup() %>% 
  # count specimens / species
  group_by(species) %>% 
  mutate(n.specimens.in.species = length(unique(specimen))) %>% 
  ungroup()
df.summary.species

# boxplot of maximum force in species
ggplot(data = df.summary.species, mapping = aes(x=species,y=max.F.specimen)) +
  geom_jitter(aes(color='blue'),alpha=0.7) +
  geom_boxplot(fill="bisque",color="black",alpha=0.3) +
  # scale_y_log10() +
  labs(x='species', y="max(F)/specimen") +
  guides(color=FALSE) +
  theme_minimal()

```


## Identify Bites
Now we come to one of the core functions of `forceR`: `find_strongest_peaks()`. This function identifies the strongest peaks of each measurement. But first, we need to create some folders to store the data and plots that are produced in all subsequent analyses of this example:

```{r eval=TRUE, warning=FALSE, message=FALSE}
# create folders to save df and results (if they do not exist yet) ---------------------------------
path.plots <- paste0(data.folder, "/plots/")
ifelse(!dir.exists(path.plots), dir.create(path.plots), "./plots already exists")
path.plots.initial_peak_finding <- paste0(data.folder, "/plots/initial_peak_finding/")
ifelse(!dir.exists(path.plots.initial_peak_finding), dir.create(path.plots), "./plots/initial_peak_finding already exists")
path.data <- paste0(data.folder, "/data/")
ifelse(!dir.exists(path.data), dir.create(path.data), "./data already exists")

```

Now we let `find_strongest_peaks()` identify all actual peak curves in all measurements:

```{r eval=FALSE, warning=FALSE, message=FALSE}
peaks.df <- find_strongest_peaks(df = df.all.200.tax, 
                                 no.of.peaks = 5, 
                                 path.plots = path.plots,
                                 print.to.pdf = TRUE)
```

The initial peak finding in the first iteration of the function `find_strongest_peaks()` identifies peaks *via* a the `initial.threshold` which is by default set to `0.05` (= 5%) of the maximum force of the respective measurement. The threshold is indicated as a green line in the plots. Bite starts are indicated as blue points, peak ends as orange points:

![](C:/Users/pruehr.EVOLUTION/Documents/forceR_bkp_2022-03-01/vignettes/figures/intial_bites.png){width=6in}

In a second iteration, the function `find_strongest_peaks()` optimizes the peak starts and ends found in the first iteration. Starting from each start, a sliding window of size `slope.length.start` (in time steps) moves backwards in time and calculates the slope within the current window. The parameter `slope.thresh.start` defines below which slope the process is stopped. The time point where the threshold is reached is treated as the actual start of that peak. The same is done in forward direction with the peak ends. Here, `slope.length.end` (in time steps) defines the sliding window size, and `slope.thresh.end` defines the slope threshold. All these settings have default values which proved as good choices for insect bites. The peak optimization will not be done for all peaks, but only for the strongest peaks per species (not per measurement!), specified by `no.of.peaks`. If fewer than `no.of.peaks` peaks are found for a species, the results table will contain fewer peaks for this species. No warning is issued.

The results are not automatically plotted. This can be done with the function `plot_peaks()`:

```{r eval=FALSE, warning=FALSE, message=FALSE}
path.plots <- paste0(data.folder, "/plots/")
plot_peaks(df.peaks = peaks.df,
           df.data = df.all.200.tax, 
           additional.msecs = 2000, 
           path.plots = path.plots, 
           print.to.pdf = TRUE)
```

The first page of the resulting PDF looks like this, showing the five strongest peaks of measurement 0981, and the strongest peak of measurement 1041. Plots of further peaks of 1041 and the peaks of the other measurements follow on the next page(s).

![](C:/Users/pruehr.EVOLUTION/Documents/forceR_bkp_2022-03-01/vignettes/figures/all_bite_curves.png){width=6in}
We strongly recommend saving `df.peaks` as a csv file on your computer and keep it. The following procedures overwrite values in this table, so the peak-finding step needs to be repeated to restore original values.

## Manual Peak Corrections
For some measurements, the automatic finding identification of peak starts and ends may have failed. Changing the parameters of `find_strongest_peaks()` might help, but in some cases, the starts and ends have to be manually defined. For these cases, the function `correct_peak()` has been written.

```{r eval=FALSE, warning=FALSE, message=FALSE}
peaks.df <- correct_peak(df.peaks = peaks.df,
                         df.data = df.all.200.tax,
                         measurement = "0981", 
                         peak = 1, 
                         additional.msecs = 500)
```

This functions prompts the user to define a new start and end of the peak. Define the `measurement` ID (as a character string) in which the peak to correct is located, and the `peak` number (as a numerical value). Both can be found in the respective title of the peak curve plot created by `plot_peaks()`. If the desired start and/or end lies outside of the plot window, increase `additional.msecs`.

![](C:/Users/pruehr.EVOLUTION/Documents/forceR_bkp_2022-03-01/vignettes/figures/manual_bite_corr.png){width=6in}


The function overwrites the start and end values of the selected peak in the selected measurement and plots the corrected peak curve: 

```
[1] "Select new peak start and end. If more than two points are selected, the operation is automatically terminated."
species                 measurements                            starts                              ends
1       A 0981; 0981; 0981; 0981; 0981  13720; 33610; 17900; 3050; 31035  15142; 34685; 19425; 4240; 32395
2       B 1041; 1041; 1041; 1041; 1041  19440; 58485; 24345; 62235; 8100  23325; 59230; 25730; 63070; 9310
3       C 1440; 1440; 1440; 1440; 1440 11290; 14165; 13280; 22655; 21615 12590; 14835; 13925; 23175; 22080
```

![](C:/Users/pruehr.EVOLUTION/Documents/forceR_bkp_2022-03-01/vignettes/figures/manual_bite_corred.png){width=6in}

```{r eval=TRUE, warning=FALSE, message=FALSE, include=F}
peaks.df <- read.csv(file.path(path.data, "2021_12_06_taxa_starts_ends.csv"))
```

## Normalize (Rescale) Peaks
In the next step, we will use `rescale_peaks()` to rescale the time and force data so that they both range from 0 to 1.

```{r, eval=TRUE, warning=FALSE, message=FALSE, include=TRUE}
peaks.df.norm <- rescale_peaks(df.peaks = peaks.df,
                               df.data = df.all.200.tax)
```

```{r eval=TRUE, warning=FALSE, message=FALSE}
peaks.df.norm
```

## Reduce to 100 Time Steps per Peak
`peaks.df.norm` now contains the same data as `df.all.200.tax`, only with the additional columns of `t.norm` and `force.norm` which contain the 
rescaled time and force data, both ranging from 0 to 1.

To reduce the data to 100 observations (time steps), we will use the function `red_peaks_100()`:

```{r eval=TRUE, warning=FALSE, message=FALSE, include=TRUE}
peaks.df.norm.100 <- red_peaks_100(df = peaks.df.norm, 
                                   path.plots = path.plots, 
                                   print.to.pdf = TRUE)
peaks.df.norm.100
```

The resulting tibble consists of 100 rows per peak, and within each peak, the `index` column runs from 1 to 100, and the `force.norm.100` column consists of the rescaled peak curves, each of whose values range from 0 to 1. Since we included five species in our dataset, the resulting tibble is 1,500 rows long (`species * peaks/species * time steps per peak = 3 * 5 * 100`). With data in this format, we could start the 1-dinemsional statistical parametric mapping workflow in `Python`, but let's structure the data within `R` with some further grouping variables before the export towards `Python`. But before that, we first find the best polynomial fits to describe all of our peak curves.


## Average Peak Curve per Species
Next, we will calculate the average peak curve per species and normalize the curves again:

```{r eval=TRUE, warning=FALSE, message=FALSE, fig.width = 6, fig.height=4}
peaks.df.100.avg <- avg_peaks(df = peaks.df.norm.100)
head(peaks.df.100.avg)

# plot averaged normalized curves per species ---------------------------------
ggplot(peaks.df.100.avg, aes(x = index , 
                             y = force.norm.100.avg, 
                             colour=species)) +
  geom_line()
```

## Fitting Polynomial Functions to the Force Curves
To find out the minimum number of coefficients that well describe all curves, we use the function `find_best_fits()`. It fits polynomial functions with 1 to 20 coefficients and uses the Akaike Information Criterion (AIC) to evaluate the goodness of the fits. A model is considered a good fit when the percentage of change from one model to the next (e.g. a model with 6 coefficients to a model with 7 coefficients) is `<= 5%`. The first four models meeting this criterion are plotted as colored graphs and the AICs of these models are visualized in a second plot for each curve. All first four coefficients per curve that fulfill the criterion are stored and in the end, a histogram of how often which coefficients were good fits is plotted as well. The function returns the numerical value of the coefficient that fulfilled the criterion of a good fit in most curves. If `print.to.pdf == TRUE`, then the plots are saved as PDFs in `path.plots`. Resulting data is saved in `path.data`.


```{r eval=FALSE, warning=FALSE, message=FALSE, include=F}
# # here!
# data.folder <- "C:/Users/pruehr.EVOLUTION/Documents/forceR_bkp_2022-03-01/vignettes/example_data"
# results.folder <- file.path(data.folder, "corrected/")
# df.all <- load_mult(results.folder)
# df.all.200 <- reduce_frq(df.all, Hz = 200, measurement.col = "filename")
# df.all.200 <- df.all.200 %>% mutate(measurement = gsub("(^[^_])*_.*", "\\1", filename))
# classifier <- tibble(measurement = c("0979", "0980", "0981", "0982", "1041", "1068", "1174", "1440"),
#                      specimen = c("a","a","b","c","d","e","f","g"),
#                      species = c("A","A","A","A","B","B","C","C"),
#                      amp = c(2, 2, 2, 2, 2, 2, 0.5, 0.5),
#                      lever.ratio = c(0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5)) %>%
#   # sort columns
#   select(species, specimen, measurement, amp, lever.ratio)
# df.all.200.tax <- y_to_force(df.all.200, 
#                          classifier,
#                          measurement.col = "measurement")
# 
# 
# path.plots <- paste0(data.folder, "/plots/")
# path.data <- paste0(data.folder, "/data/")
# path.data.manual.peak.start.end.logs <- paste0(path.data, "/manual.peak.start.end.logs/")
# peaks.df <- read.csv(file.path(path.data, "2021_12_06_taxa_starts_ends.csv"))
# peaks.df.norm <- rescale_peaks(df.peaks = peaks.df,
#                                df.data = df.all.200.tax)
# peaks.df.norm.100 <- red_peaks_100(df = peaks.df.norm, 
#                                    path.plots = path.plots, 
#                                    print.to.pdf = TRUE)
# peaks.df.100.avg <- avg_peaks(peaks.df.norm.100)
# 
# best.fit.poly <- find_best_fits(df = peaks.df.100.avg,
#                                 print.to.pdf = TRUE,
#                                 path.data,
#                                 path.plots)
```

```{r eval=TRUE, warning=FALSE, message=FALSE}
best.fit.poly <- find_best_fits(df = peaks.df.100.avg, 
                                print.to.pdf = TRUE, 
                                path.data, 
                                path.plots)
```
```{r eval=TRUE, warning=FALSE, message=FALSE}
best.fit.poly
```

The number of coefficients that most often was under the first 4 models that were followed by an AIC-change `<= 5%` is 10:

![](C:/Users/pruehr.EVOLUTION/Documents/forceR_bkp_2022-03-01/vignettes/figures/coeff_graphs.png){width=4in}

Given the fact that we only analyzed three species (`n = 3 species * 4 good coefficients = 12`) in this example, the histogram looks a bit scarce:

![](C:/Users/pruehr.EVOLUTION/Documents/forceR_bkp_2022-03-01/vignettes/figures/best_coeffs.png){width=4in}

If more species are analyzed, this may rather look like this:

![](C:/Users/pruehr.EVOLUTION/Documents/forceR_bkp_2022-03-01/vignettes/figures/best_coeffs_more.png){width=4in}

### Convert Curves to Polynomial Models
In the last step of this vignette, we convert the average peak curve shape into polynomial models in which all have the same amount of coefficients (defined by the previously identified `best.fit.poly`):

```{r eval=TRUE, warning=FALSE, message=FALSE, include=F}
models <- peak_to_poly(peaks.df.100.avg, 
                       best.fit.poly)
```

```{r eval=FALSE, warning=FALSE, message=FALSE}
models <- peak_to_poly(peaks.df.100.avg, 
                       best.fit.poly)
```

```{r eval=TRUE, warning=FALSE, message=FALSE, fig.width = 6, fig.height=4}
# plot all polynomial models
plot(predict(models[[1]]), 
     ylim = c(0,1), 
     type = "n",
     xlab="t", 
     ylab="F")

for(i in 1:length(models)){
  lines(predict(models[[i]]), 
        lwd=1, 
        col=c("red", "green", "blue")[i]) # col=as.numeric(as.factor(names(models)))
}
```


## Well done!
Congratulations - you have made it to the end of this vignette. We hope this package helps you with your data. In case of any remarks or questions, please feel free to contact Peter T. Rühr at `peter.ruehr at gmail.com`.

## `forceR` Workflow Example
Below we have compiled a self-sufficient `R` script to run all functions of `forceR` after file loading and, if necessary, drift corrections, are finished. Instead of loading data from real *in vivo* measurements, we simulate bite series with the function `simulate_bites()` and store it in `df.all`. The variable names used are the same as used in the rest of this vignette.

```{r eval=FALSE, warning=FALSE, message=FALSE}
require(gsheet)
require(ggplot2)
require(grid)
require(reshape)
require(dplyr)

# PREPARTIONS ####
# set seed for randomization so results are reproducible
set.seed(1)

# DEFINE SOME FUNCTIONS
today <- function(){
  date.string <- gsub("-", "_", substring(as.character(as.POSIXct(Sys.time())), 1, 10))
  return(date.string)
}

# plot linear regression
plot.linear.regression <- function(x, y,
                                   logarithmic = F,
                                   x.axis.label = "x",
                                   title = NULL,
                                   x.lim = NULL, y.lim = NULL){
  if(logarithmic == "10"){x <- log10(x); y <- log10(y)}
  if(logarithmic == "e"){x <- log(x); y <- log(y)}
  lin.mod <- lm(y ~ x)
  lin.mod.sum <- summary(lin.mod)
  lin.mod.r2 <- lin.mod.sum$adj.r.squared
  lin.mod.p <- lin.mod.sum$coefficients[2,4]
  lin.mod.intercept <- lin.mod$coefficients[1]
  lin.mod.slope <- lin.mod$coefficients[2]
  lin.mod.label.r2 <- bquote(italic(R)^2 == .(format(lin.mod.r2, digits = 3)))
  if(lin.mod.p > 0.05) {lin.mod.p.ast <- lin.mod.p}
  if(lin.mod.p <= 0.05 & lin.mod.p > 0.01) {lin.mod.p.ast <- "*"}
  if(lin.mod.p <= 0.01 & lin.mod.p > 0.001) {lin.mod.p.ast <- "**"}
  if(lin.mod.p <= 0.001) {lin.mod.p.ast <- "***"}
  
  lin.mod.label.p <- bquote(italic(p) == .(lin.mod.p.ast))
  
  if(is.null(xlim)){
    x.lim = c(min(x), max(x))
  }
  if(is.null(ylim)){
    y.lim = c(min(y), max(y))
  }
  
  if(lin.mod.p >= 0.001) p.print <- paste0("p = ", round(lin.mod.p, 3))
  if(lin.mod.p < 0.001) p.print <- "p < 0.001"
  plot(x, y, pch = 16, xlab = paste0(x.axis.label, ": ", p.print,
                                     "; R2 = ", round(lin.mod.r2,3),
                                     "; m = ",  round(lin.mod.slope,3),
                                     "; y = ",  round(lin.mod.intercept,3)),
       xlim = x.lim, ylim = y.lim)
  
  abline(lin.mod, col = "red")
  
  if(!is.null(title)){
    title(main = title)
  }
  print(x.axis.label)
  print(paste0(p.print))
  print(paste0("R2 = ", round(lin.mod.r2,3)))
  print(paste0("m = ", round(lin.mod.slope,3)))
  print(paste0("y = ", round(lin.mod.intercept,3)))
  
  res <- tibble(p = lin.mod.p,
                r2 = lin.mod.r2,
                intercept = lin.mod.intercept,
                slope = lin.mod.slope)
  return(res)
}

# add leading zeros function
add_eading_zeros <- function(number, length){
  require(stringr)
  str_pad(number, length, pad = "0")
}
# /DEFINE SOME FUNCTIONS


# create classifier and store as classifier_original for later
classifier <- classifier_original <- tibble(bite.type = c(rep("sinosoidal", 5), rep("plateau", 3), rep("inter", 4)),
                                            peak.position = c("early","early","center","late","late","center","center","center","center","center","early","late"),
                                            species = NA,
                                            specimen = NA,
                                            measurement = NA,
                                            body_mass = NA,
                                            force_in = NA,
                                            length.of.bite = 4000,
                                            peak.pos = c(20, 25, 50, 65, 70, rep(NA, 7)),
                                            slope.perc.starts = c(rep (NA, 5), 10,15,20,30,40,20,50),
                                            slope.perc.ends = c(rep (NA, 5), 10,15,20,30,40,50,20),
                                            type = c(rep("sin", 5), rep("plat", 7)),
                                            no.of.bites = 7,
                                            amp = 1,
                                            lever.ratio = 1,
                                            length.of.series = 35000)


# amend classifier
classifier <- classifier %>%
  mutate(no = row_number())
classifier

# define maximum forces per bite simulation type and add additional rows to classifier
forces <- c(1, 5, 15)
for(i in 1:nrow(classifier)){
  classifier$force_in[i] <- forces[1]
  classifier <- classifier %>%
    add_row(classifier[i, ])
  classifier$force_in[nrow(classifier)] <- forces[1]
  for(f in 2:3){
    classifier <- classifier %>%
      add_row(classifier[i, ])
    classifier$force_in[nrow(classifier)] <- forces[f]
    classifier <- classifier %>%
      add_row(classifier[i, ])
    classifier$force_in[nrow(classifier)] <- forces[f]
  }
}

# arrange classifier by original bite.type and remove bite.typeing column
classifier <- classifier %>%
  arrange(no) %>%
  select(-no)

# create vector of species names and add to classifier
species.names <- NULL
for(i in 1:(nrow(classifier)/2)){
  species.names <- c(species.names,
                     rep(paste0("S", add_eading_zeros(i, 2)), 2))
}
classifier$species <- species.names

classifier$specimen <- paste0("s", add_eading_zeros(1:nrow(classifier), 3))
classifier$measurement <- paste0("m", add_eading_zeros(1:nrow(classifier), 3))

# assign body mass according to the maximum force
classifier$body_mass[classifier$force_in == forces[1]] <- 1
classifier$body_mass[classifier$force_in == forces[2]] <- 6
classifier$body_mass[classifier$force_in == forces[3]] <- 25

# add jitter to force and body mass to replicate biological variation
for(i in 1:nrow(classifier)){
  classifier$force_in[i] <- round(classifier$force_in[i] +
                                    ((rnorm(1, -0.2, 0.2)) * classifier$force_in[i]), 2)
  classifier$body_mass[i] <- round(classifier$body_mass[i] +
                                     ((rnorm(1, -0.2, 0.2)) * classifier$body_mass[i]), 2)
}
# /amend classifier

# get overview of input data before simulating bite series
BFQ.regression_in <- plot.linear.regression(x = classifier$body_mass,
                                            y = classifier$force_in,
                                            logarithmic = "10",
                                            x.axis.label = "body mass")

# jitter for variation in maximum bite force within a bite series
# this was set to 0 when checking if the package finds the correct max. force values
# and to 15 to increase bite shape diversity when checking if the package can tell the different
# bite shapes apart
max.y.jit = 15 # 0 15

# jitter to make the bite curve more unstable
# this was set to 0 when checking if the package finds the correct max. force values
# and to 1 to increase bite shape diversity when checking if the package can tell the different
# bite shapes apart
jit = 1 # 0 1

# create tibble with simulated time series with different
# bite characteristics for each measurement, specimen and species
path.plots <- "Z:/PAPERS/PTR_Bite force METHODS/R/package_tests/plots/"
print(paste0("Saving plots at ", path.plots, "/", today(),"_bite_series.pdf..."))
# pdf(paste0(path.plots, "/", today(),"_bite_series.pdf..."), onefile = TRUE, paper = "a4", height = 14)
par(mfrow = c(3,2))
df.all <- NULL
for(i in 1:nrow(classifier)){
  df.curr <- simulate_bites(no.of.bites = 5,
                            length.of.bite = classifier$length.of.bite[i],
                            length.of.series = 5*classifier$length.of.bite[i] + 5*1000,
                            max.y = classifier$force_in[i],
                            max.y.jit = max.y.jit,
                            jit = jit,
                            peak.pos = classifier$peak.pos[i],
                            slope.perc.start <- classifier$slope.perc.starts[i],
                            slope.perc.end <- classifier$slope.perc.ends[i],
                            bite.type = classifier$type[i],
                            plot = TRUE)
  
  # add measurement number to df.curr
  df.curr <- df.curr %>%
    mutate(measurement = classifier$measurement[i])
  
  # add current sumulated bite series to df.all
  df.all <- rbind(df.all, df.curr)
}
# dev.off()

# remove columns from classifier that were only used during bite series simulation
classifier <- classifier %>%
  select(-c(jit, length.of.bite, peak.pos,
            slope.perc.starts, slope.perc.ends,
            type, no.of.bites))


# forceR WORKFLOW AFTER FILE LOADING ####
# please see the package vignette for details on how to load files.

# reduce sampling frequency to 200 Hz
df.all.200 <- reduce_frq(df.all, Hz = 200,
                         measurement.col = "measurement")

# convert y values to force and add measurement columns from classifier info (df.all)
df.all.200.tax <- y_to_force(df = df.all.200, 
                             classifier = classifier,
                             measurement.col = "measurement")

# summarize force data per specimen
var1 = "measurement"
var2 = "specimen"
df.summary.specimen <- summarize_measurements(df = df.all.200.tax,
                                              var1, var2)

# add body mass from classifier:
df.summary.specimen <- df.summary.specimen %>%
  left_join(classifier %>%
              select(measurement, body_mass))

# boxplot of maximum force of all specimens
ggplot(data = df.summary.specimen, mapping = aes(x=specimen,y=max.F.measurement)) +
  geom_jitter(color='blue',alpha=0.5, width = 0.2) +
  geom_boxplot(fill="blue",color="black",alpha=0.1) +
  # scale_y_log10() +
  labs(x='specimen', y="max(F)/specimen") +
  guides(color=FALSE) +
  theme_minimal()

# boxplot of maximum force in species
ggplot(data = df.summary.species, mapping = aes(x=species,y=max.F.specimen)) +
  geom_jitter(color='blue',alpha=0.5, width = 0.2) +
  geom_boxplot(fill="blue",color="black",alpha=0.1) +
  # scale_y_log10() +
  labs(x='species', y="max(F)/specimen") +
  guides(color=FALSE) +
  theme_minimal()

# Summarize to species-wise info
# We are not using the summarize_measurements() functions because this would ignore
# the fact the some measurements may come from the same specimen, but we only want
# to consider on maximum force value per specimen and not on per measurement.
df.summary.species <- df.summary.specimen %>%
  # find max Fs of species
  group_by(species) %>%
  # calculate force values for each species
  mutate(max.F.species = max(max.F.specimen),
         mean.F.species = round(mean(max.F.specimen),6),
         sdv.max.F.species = sd(max.F.specimen)) %>%
  ungroup() %>%
  # count specimens / species
  group_by(species) %>%
  mutate(n.specimens.in.species = length(unique(specimen))) %>%
  # add body mass from classifier
  left_join(classifier %>%
              select(measurement, body_mass)) %>%
  # calculate mean body mass per species
  group_by(species) %>%
  mutate(body_mass.species = mean(body_mass)) %>%
  ungroup()

# calculate and plot the regressions of known (simulation inputs) and extracted forces over body length
# pdf(file = paste0(path.plots, today(), "_regressions.pdf"),
#     paper = "special", width = 5, height = 12)
par(mfrow=c(3,1))
# Specimen-wise regression of known maximum force values over body mass
BFQ.regression_in <- plot.linear.regression(x = classifier$body_mass,
                                            y = classifier$force_in,
                                            logarithmic = "10",
                                            x.axis.label = "body mass")

# Specimen-wise regression of extracted maximum force data over body mass
BFQ.regression.specimen <- plot.linear.regression(x = df.summary.specimen$body_mass,
                                                  y = df.summary.specimen$max.F.specimen,
                                                  logarithmic = "10",
                                                  x.axis.label = "body mass")

# Species-wise regression of extracted maximum force data over body mass
BFQ.regression.species <- plot.linear.regression(x = df.summary.species$body_mass.species,
                                                 y = df.summary.species$mean.F.species,
                                                 logarithmic = "10",
                                                 x.axis.label = "body mass")

# dev.off()
par(mfrow=c(1,1))

# calculate differences between known and extracted maximum forces
force.comp <- classifier %>%
  select(measurement, force_in) %>%
  left_join(df.summary.specimen %>%
              select(measurement, max.F.measurement)) %>%
  mutate(diff.abs = max.F.measurement - force_in,
         diff.perc = diff.abs*100/force_in) %>%
  arrange(diff.perc)

# violin plot of differences between known and extracted maximum for per specimen [in %]
ggplot(data = force.comp, mapping = aes(x= 1, y=diff.perc)) +
  geom_jitter(color='blue',alpha=0.7, width = 0.1) +
  geom_violin(fill="blue",color="black",alpha=0.1) +
  # scale_y_log10() +
  labs(x='', y="diff. pred/actual [%") +
  guides(color=FALSE) +
  theme_minimal()

# extract coefficients of species-wise regression
# to calculate bite force quotient (BFQ; Wroe et al. 2005, Christiansen & Wroe 2007)
regression.m <- BFQ.regression.species$slope
regression.y <- BFQ.regression.species$intercept

# calculate BFQ per species
df.summary.species$BFQ.body_mass <- 100*(df.summary.species$mean.F.species/
                                           10^(regression.m * log10(df.summary.species$body_mass) + regression.y))

# plot species-wise BFQ as histogram
hist(df.summary.species$BFQ.body_mass, breaks = 25)

# INDIVIDUAL BITE CURVE FINDING
# we have set all plotting parameters so that they plot to the R plot device (print.to.pdf = FALSE)
# and not to PDFs (print.to.pdf = TRUE) so that results can be observed in real time.
# find five strongest peaks per species
peaks.df <- find_strongest_peaks(df = df.all.200.tax,
                                 no.of.peaks = 5,
                                 path.plots = path.plots,
                                 print.to.pdf = FALSE) # TRUE

# save plots of every peak in a PDF
plot_peaks(df.peaks = peaks.df,
           df.data = df.all.200.tax,
           additional.msecs = 2000,
           path.plots = path.plots,
           print.to.pdf = FALSE) # TRUE

# rescale bites
peaks.df.norm <- rescale_peaks(df.peaks = peaks.df,
                               df.data = df.all.200.tax)

# check if rescaling worked: both following lines should print 1
max(peaks.df.norm$t.norm)
max(peaks.df.norm$force.norm)

# reduce to 100 observations per bite
peaks.df.norm.100 <- red_peaks_100(df = peaks.df.norm,
                                   path.plots = path.plots,
                                   print.to.pdf = FALSE) # TRUE

# get average bite curve per species
peaks.df.100.avg <- avg_peaks(df = peaks.df.norm.100)

# find best polynomial degree to describe all average curves
best.fit.poly <- find_best_fits(df = peaks.df.100.avg,
                                path.plots = path.plots,
                                print.to.pdf = FALSE) # TRUE

# convert species-wise average curves to polynomial models
models <- peak_to_poly(peaks.df.100.avg,
                       best.fit.poly)

# convert models to PCA input data
pca.data <- sapply(models, function(x){
  cbind(x[[1]]) # extract the common coefficients
})

# transpose PCA data (coefficients of polynomial models)
pca.data <- t(pca.data)

# perform PCA
PCA.bite.shape <- prcomp(pca.data)
summary(PCA.bite.shape)

# store and principal component scores in a tibble
PCA.res <- as_tibble(PCA.bite.shape$x[,1:3]) %>%
  mutate(species = rownames(PCA.bite.shape$x)) %>%
  left_join(classifier %>%
              select(bite.type, peak.position, species)) %>%
  select(bite.type, peak.position, species, PC1, PC2) %>%
  distinct(species, .keep_all = TRUE) %>%
  dplyr::rename(peak.position = peak.position,
                bite.type = bite.type)

# plot PC1 against PC2
ggplot(data = PCA.res, aes(x = PC1, y = PC2, col = bite.type)) +
  geom_point()

# pdf(file = paste0(path.plots, today(), "_PCA_bite_shape.pdf"),
#     paper = "a4r", width = 29, height = 21) # , height = 14
ggplot(data = PCA.res, aes(x = PC1, y = PC2, col = peak.position)) +
  geom_point()
# dev.off()

# plot PC1 against PC1 with bite shapes as insets
# pdf(file = paste0(path.plots, today(), "_PCA_bite_shape_w_curves.pdf"),
#     paper = "a4r", width = 20, height = 21) # , height = 14
# create main PCA plot
main_plot <- ggplot(data = PCA.res, aes(x = PC1, y = PC2, col = peak.position)) +
  geom_point() +
  theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank(),
        legend.position = "none")
print(main_plot)

# create an and plot an inlet with the bite shape
# of the first bite of the first
# simulation settings of each specimen
species_to_plot <- paste0("S", seq(1, nrow(PCA.res), 3))
for(i in seq(1, nrow(PCA.res), 3)){
  curr.PC1 <- PCA.res$PC1[i]
  curr.PC2 <- PCA.res$PC2[i]
  curr.species <- PCA.res$species[i]
  curr.bite.data <- peaks.df.100.avg %>%
    filter(species == curr.species)
  
  inset_plot <- ggplot(curr.bite.data, aes(index, force.norm.100)) +
    geom_line() +
    theme(axis.title.x=element_blank(),
          axis.text.x=element_blank(),
          axis.ticks.x=element_blank(),
          axis.title.y=element_blank(),
          axis.text.y=element_blank(),
          axis.ticks.y=element_blank(),
          plot.margin=grid::unit(c(0,0,0,0), "null"),
          panel.background = element_rect(fill = 'white', colour = 'white')) +
    theme(aspect.ratio=1)
  
  #A viewport taking up a fraction of the plot area
  vp <- viewport(width = 0.1, height = 0.1,
                 x = (curr.PC1-min(PCA.res$PC1))/diff(range(PCA.res$PC1)),
                 y =(curr.PC2-min(PCA.res$PC2))/diff(range(PCA.res$PC2)))
  
  print(inset_plot, vp = vp)
}
# dev.off()


}
